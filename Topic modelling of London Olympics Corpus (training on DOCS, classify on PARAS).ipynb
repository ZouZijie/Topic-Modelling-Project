{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build A LDA Model of 2012 London Olympics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Packages Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet  import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as py\n",
    "import string\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore, Phrases, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.phrases import Phraser\n",
    "# libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parameters Setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 2                             # Define which columns are used for topic modelling.\n",
    "coherence_type = 'c_v'       # Define coherence type\n",
    "min_len = 4                       # Define the short word length\n",
    "no_below =  2                   #a threshold filtering out number of occurrence of the tokens among documents  no larger than 5\n",
    "no_above = 0.9                 #the portion of a word in total corpus size\n",
    "chunksize = 2000              #Number of documents to be used in each training chunk\n",
    "eval_every = None             # Don't evaluate model perplexity, takes too much time.\n",
    "num_topics =  20              #The number of requested latent topics to be extracted from the training corpus.\n",
    "num_passes = 20              #Number of passes through the corpus during training\n",
    "iterations = 100                #Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
    "alpha = 50.0/num_topics   #expresses our a-priori belief for each topics’ probability\n",
    "#eta = 'auto'     \n",
    "eta =      0.01                  #A-priori belief on word probability\n",
    "random_state  = 54321     #Either a randomState object or a seed to generate one. Useful for reproducibility.\n",
    "lemmatization = 1                  #Whether implement corpus lemmatization 2:Lemmatizer (without POS tags) 1:Lemmatizer (with POS tags) 0:no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Corpus Reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Olympics-ALL-DOCS.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Olympics-All-DOCS/NewChunk1/Examination_of_Wit...</td>\n",
       "      <td>Examination of Witnesses   SEPTEMBER   MS BAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Olympics-All-DOCS/NewChunk1/Examination_of_Wit...</td>\n",
       "      <td>Examination of Witnesses   SEPTEMBER   MS BAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Olympics-All-DOCS/NewChunk1/Examination_of_Wit...</td>\n",
       "      <td>Examination of Witnesses   SEPTEMBER   MS BAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Olympics-All-DOCS/NewChunk1/Jan_2003_-_Qs_1-19...</td>\n",
       "      <td>Select Committee on Culture Media and Sport M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Olympics-All-DOCS/NewChunk1/Jan_2003_-_Qs_100-...</td>\n",
       "      <td>Select Committee on Culture Media and Sport M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>What had not happened at that stage was the d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>eached settlement anyway and we havent been so...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>gret that  Examination of Witnesses  Witnesses...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>will have been contacted by Join In Trust I ho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>before the Committee on  December I undertook...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0                                                    1   \n",
       "1                                                    2   \n",
       "2                                                    3   \n",
       "3                                                    4   \n",
       "4                                                    5   \n",
       "..                                                 ...   \n",
       "290   What had not happened at that stage was the d...   \n",
       "291  eached settlement anyway and we havent been so...   \n",
       "292  gret that  Examination of Witnesses  Witnesses...   \n",
       "293  will have been contacted by Join In Trust I ho...   \n",
       "294   before the Committee on  December I undertook...   \n",
       "\n",
       "                                                     1  \\\n",
       "0    Olympics-All-DOCS/NewChunk1/Examination_of_Wit...   \n",
       "1    Olympics-All-DOCS/NewChunk1/Examination_of_Wit...   \n",
       "2    Olympics-All-DOCS/NewChunk1/Examination_of_Wit...   \n",
       "3    Olympics-All-DOCS/NewChunk1/Jan_2003_-_Qs_1-19...   \n",
       "4    Olympics-All-DOCS/NewChunk1/Jan_2003_-_Qs_100-...   \n",
       "..                                                 ...   \n",
       "290                                                NaN   \n",
       "291                                                NaN   \n",
       "292                                                NaN   \n",
       "293                                                NaN   \n",
       "294                                                NaN   \n",
       "\n",
       "                                                     2  \n",
       "0     Examination of Witnesses   SEPTEMBER   MS BAR...  \n",
       "1     Examination of Witnesses   SEPTEMBER   MS BAR...  \n",
       "2     Examination of Witnesses   SEPTEMBER   MS BAR...  \n",
       "3     Select Committee on Culture Media and Sport M...  \n",
       "4     Select Committee on Culture Media and Sport M...  \n",
       "..                                                 ...  \n",
       "290                                                NaN  \n",
       "291                                                NaN  \n",
       "292                                                NaN  \n",
       "293                                                NaN  \n",
       "294                                                NaN  \n",
       "\n",
       "[295 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot most frequent terms\n",
    "def freq_words(x, terms = 30):\n",
    "    all_words = ' '.join(['%s' %text for text in x])\n",
    "    all_words = all_words.split()\n",
    "    \n",
    "    fdist = FreqDist(all_words)\n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n",
    "    \n",
    "    # selecting top 20 most frequent words\n",
    "    d = words_df.nlargest(columns=\"count\", n = terms) \n",
    "    plt.figure(figsize=(20,5))\n",
    "    ax = sns.barplot(data=d, x= \"word\", y = \"count\")\n",
    "    ax.set(ylabel = 'Count')\n",
    "    plt.show()\n",
    "\n",
    "print('Word number: %d'% (len(df[col])))\n",
    "freq_words(df[2],10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the corpus \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for i in range (len(df)):\n",
    "    df[col][i] = df[col][i].lower()\n",
    "    df[col][i] = tokenizer.tokenize(df[col][i])\n",
    "\n",
    "print('Tokens number: %d'% (len(Dictionary(df[col]))))\n",
    "freq_words(df[col],10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the number\n",
    "df[col] = [ [token for token in doc if not token.isnumeric()] for doc in df[col]]\n",
    "\n",
    "print('Tokens number after removing number: %d'% (len(Dictionary(df[col]))))\n",
    "freq_words(df[col],10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reomve stop words\n",
    "stop_words = stopwords.words('english')\n",
    "df[col] = [ [token for token in doc if not token in stop_words] for doc in df[col] ]\n",
    "print('Tokens number after removing stop words: %d'% (len(Dictionary(df[col]))))\n",
    "freq_words(df[col],10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove short words (length< min_len)\n",
    "df[col] = [ [token for token in doc if not len(token) < min_len] for doc in df[col] ]\n",
    "print('Tokens number after removing short words: %d'% (len(Dictionary(df[col]))))\n",
    "freq_words(df[col],10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus Lemmatization\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def lemmatize_sentence(sentence):\n",
    "    res = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, pos in pos_tag(sentence):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "\n",
    "    return res\n",
    "\n",
    "if lemmatization == 1:\n",
    "    for i in range (len(df)):\n",
    "        df[col][i] = lemmatize_sentence(df[col][i])\n",
    "    print('Lemmatization with POS tags...')\n",
    "\n",
    "if lemmatization == 2:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[col] = [ [lemmatizer.lemmatize(token) for token in doc ] for doc in df[col] ]\n",
    "    print('Lemmatization without POS tags...')\n",
    "\n",
    "if lemmatization == 0:\n",
    "    print('Cancel lemmatization...')\n",
    "\n",
    "print('Tokens number after lemmatization: %d'% (len(Dictionary(df[col]))))\n",
    "freq_words(df[col],10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all punctuations\n",
    "#punctuation_string = string.punctuation\n",
    "#for j in range (len(df)):\n",
    "#    for i in punctuation_string:\n",
    "#        df[col][j] = str(df[col][j]).replace(i, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make bigram\n",
    "bigrams_phases = Phrases(df[col], min_count = 10)\n",
    "for i in range(len(df[col])):\n",
    "    for token in bigrams_phases[df[col][i]]:\n",
    "        if '_' in token:\n",
    "            df[col][i].append(token)            \n",
    "\n",
    "print('Tokens number after adding bigram: %d'% (len(Dictionary(df[col]))))\n",
    "freq_words(df[col],10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 The LDA Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary\n",
    "LOdict = Dictionary(df[col])\n",
    "\n",
    "print('Tokens number after creating dictionary: %d'% (len(LOdict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove  words occur less than no_below documents and more then no_above/documents\n",
    "LOdict.filter_extremes(no_below = no_below, no_above = no_above)\n",
    "\n",
    "print('Tokens number after tf-idf: %d'% (len(LOdict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of word for corpus\n",
    "LOcorpus = [LOdict.doc2bow(doc) for doc in df[col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "id2word = LOdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buila a LDA model for LONDON OLYPLMICS\n",
    "LOldamodel = LdaModel(\n",
    "    corpus = LOcorpus,\n",
    "    id2word = id2word,\n",
    "    chunksize = chunksize,\n",
    "    alpha = alpha,\n",
    "    eta = eta,\n",
    "    iterations = iterations,\n",
    "    num_topics = num_topics,\n",
    "    passes = num_passes,\n",
    "    eval_every = eval_every,\n",
    "    random_state = random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model = LOldamodel, texts = df[col], dictionary = LOdict, coherence=coherence_type)\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score ', coherence_type,'=', coherence_lda, ' with number of topics=', num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_score = pd.DataFrame(pd.read_excel(\"topic coherence.xlsx\"))\n",
    "tc = sns.lineplot(x = 'Topics Number', y = 'Score', hue = 'Topics Coherence Type', data = tc_score)\n",
    "tc.set(title = 'Topic Coherence Score')\n",
    "plt.xticks([20,21,22,23,24,25,26,27,28,29,30])\n",
    "#tc.set_xticks(range(len(tc_score))) # <--- set the ticks first\n",
    "#tc.set_xticklabels(['20','21','22','23','24','25','26','27','28','29','30'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Corpus Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a classifier to classify each paragraph of London Olympics Corpus\n",
    "classifer = list()\n",
    "for d in df[col]:\n",
    "    bow = LOdict.doc2bow(d)\n",
    "    belong = LOldamodel.get_document_topics(bow,per_word_topics=False)\n",
    "    classifer.append(belong)\n",
    "    \n",
    "classiferdf = pd.DataFrame(classifer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform London Olympics Corpus to Dataframe\n",
    "LOdf = pd.DataFrame(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the Dataframe of the biggest distribution topics of each paragraph\n",
    "MTopics =  list()\n",
    "for i in LOldamodel.get_document_topics(LOcorpus)[:]:\n",
    "    listj=[]\n",
    "    for j in i:\n",
    "        listj.append(j[1])\n",
    "    bz=listj.index(max(listj))\n",
    "    #print(i[bz][0],i,listj,listj.index(max(listj)))\n",
    "    #print(i[bz][0])\n",
    "    MTopics.append(bz)\n",
    "\n",
    "MTopics = pd.DataFrame(MTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integrate dataframe of orgrinal corpus, classifier, and the biggest distribution of each paragraph\n",
    "classifer_document = pd.concat([LOdf,MTopics,classiferdf],axis=1)\n",
    "classifer_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output final dataframe\n",
    "classifer_document.columns = [\"Corpus\",\"Biggest distribution topic\",\"T0%\",\"T1%\",\"T2%\",\"T3%\",\"T4%\",\"T5%\",\"T6%\",\"T7%\",\"T8%\",\"T9%\",\"T10%\",\n",
    "                             \"T11%\",\"T12%\",\"T13%\",\"T14%\",\"T15%\",\"T16%\",\"T17%\",\"T18%\",\"T19%\"\n",
    "                             ]\n",
    "\n",
    "classifer_document.to_csv(path_or_buf = 'C:/Users/jason/Documents/Exeter University/business project/code/output/classifer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output topics distribution of whole corpus\n",
    "topic_distribution = pd.DataFrame(LOldamodel.print_topics(num_topics = num_topics))\n",
    "topic_distribution.columns = [\"Topic index\",\"Topic Distribution\"]\n",
    "topic_distribution\n",
    "topic_distribution.to_csv(path_or_buf = 'C:/Users/jason/Documents/Exeter University/business project/code/output/topic distribution.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The LDA Model Visulization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Packages Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensim_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(LOldamodel, LOcorpus, id2word)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
